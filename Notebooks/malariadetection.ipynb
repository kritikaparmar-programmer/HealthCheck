{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MALARIA DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at how we are gonna make our model \n",
    "\n",
    "#### Step 1:  Loading and Splitting of the Dataset\n",
    "\n",
    "- The first step is to load the data and scaling the images to binary 0 and 1 from Parasitized and Uninfected.\n",
    "- Then we will resize the images to 50 x 50 \n",
    "- After that suffling of the images before train-test-split and converting the images to a single numpy array\n",
    "- Splitting the data \n",
    "- Converting the type of X_train and X_valid to float32\n",
    "- Add then One Hot Encoding on y\n",
    "\n",
    "\n",
    "#### Step 2:  Building the CNN model\n",
    "- The CNN model is one of the efficient nueral networks for images and performing classifications. We will use tf.keras to build the CNN model.\n",
    "- We will build a Sequential CNN model.\n",
    "- We will build a CNN Layer followed by MaxPooling layer which is later followed by BatchNormalisation to normalize the previous layer's output and implement the Dropout regularization. After that we will use Flatten to the outputs. Then the last layer that has function Softmax is the output layer.\n",
    "- Finally we have to compile the CNN model. We will use optimizer called Adam then will apply the loss function as categorical_crossentropy and an evaluation metric as accuracy.\n",
    "- Next step is to use the fit function, to train our convolutional neural network (CNN) with X_train and y_train. Lets set the total amounts of epochs as 25 epochs, which is essentially 25 cycles or iterations of the full dataset including a batch size of 120.\n",
    "\n",
    "####  Step 3 : Predictions and Testing of the Model\n",
    "- After this we will predict and do evaluation on the builded model. \n",
    "- The last step will be to test our model on the HOLDOUT DATASET and making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-99125dfa07a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# importing the libraries for loading data and visualisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# importing the libraries for loading data and visualisation\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "\n",
    "# import for train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import for One Hot Encoding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# importing libraries for Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "\n",
    "# importing libraries for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Train-Test-Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data of images and setting their labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "Parasitized = os.listdir(\"../input/cell-images-for-detecting-malaria/cell_images/Parasitized/\")\n",
    "\n",
    "for a in Parasitized:\n",
    "    \n",
    "    try:\n",
    "        image = cv2.imread(\"../input/cell-images-for-detecting-malaria/cell_images/Parasitized/\" + a)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(0)\n",
    "    \n",
    "    except AttributeError:\n",
    "        print(\"\")\n",
    "\n",
    "Uninfected = os.listdir(\"../input/cell-images-for-detecting-malaria/cell_images/Uninfected/\")\n",
    "\n",
    "for b in Uninfected:\n",
    "\n",
    "    try:\n",
    "        image = cv2.imread(\"../input/cell-images-for-detecting-malaria/cell_images/Uninfected/\" + b)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(1)\n",
    "    \n",
    "    except AttributeError:\n",
    "        print(\"\")\n",
    "\n",
    "# Creating single numpy array of all the images and labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('Cells : {} and labels : {}'.format(data.shape , labels.shape))\n",
    "\n",
    "# lets shuffle the data and labels before splitting them into training and testing sets\n",
    "n = np.arange(data.shape[0])\n",
    "np.random.shuffle(n)\n",
    "data = data[n]\n",
    "labels = labels[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the dataset into the Training set and Test set\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data, labels, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print('Train data shape {} ,Test data shape {} '.format(X_train.shape, X_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')  \n",
    "X_valid = X_valid.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding \n",
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model\n",
    "classifier = Sequential()\n",
    "\n",
    "# CNN layers\n",
    "classifier.add(Conv2D(32, kernel_size=(3, 3), input_shape = (50, 50, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(BatchNormalization(axis = -1))\n",
    "classifier.add(Dropout(0.5))   # Dropout prevents overfitting\n",
    "\n",
    "classifier.add(Conv2D(32, kernel_size=(3, 3), input_shape = (50, 50, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(BatchNormalization(axis = -1))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(BatchNormalization(axis = -1))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "classifier.add(Dense(units=2, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = classifier.fit(X_train, y_train, batch_size=120, epochs=15, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test_Accuracy: {:.2f}%\".format(classifier.evaluate(X_valid, y_valid)[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Summary of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation and Testing of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets do our first prediction using predict and predict X_valid and store in y_pred variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to categorical values \n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_valid = np.argmax(y_valid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Score: ', accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation of the CNN model by Plotting Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Confusion Matrix\n",
    "conf = confusion_matrix(y_valid, y_pred)\n",
    "sns.heatmap(conf, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"malaria-model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
